# mutimodalfusion

## 模型详细架构与输入输出说明

我们的模型旨在利用 Amazon Reviews 2023 数据集中的三种模态（文本、图像、图）为用户生成个性化的 Top‑K 商品推荐。整体流程如下：对于每个目标用户，我们首先从该用户的历史交互（评论过的商品）中提取其多模态偏好，然后与候选商品的多模态特征进行匹配，最终输出最相关的 K 个商品。下面按模块详细说明每一步的输入、输出及其在数据集上的具体实现。

---

### 1. 数据预处理与模态对齐

**输入**：Amazon Reviews 2023 原始数据（JSON / Parquet 格式），包含：
- 评论表：`reviewerID`, `asin`, `reviewText`, `overall`, `unixReviewTime`
- 商品元数据表：`asin`, `title`, `description`, `image_url`, `also_bought` (list of ASINs)

**处理步骤**：
- 筛选三个商品类别（例如 Electronics, Clothing, Home）以保证数据量适中且模态覆盖率高。
- 对每个 `asin`，下载其对应图片（若无图片则标记为缺失）。
- 将 `also_bought` 列表转换为物品‑物品无向图，边权重为共现次数。
- 按时间顺序划分训练/验证/测试集：以用户最后一次交互时间点为界，之前的交互作为历史，之后的交互作为预测目标。

**输出**：结构化数据，每条记录包含：
- 用户ID
- 商品ID
- 用户历史评论文本（列表）
- 用户历史商品图片（列表）
- 商品标题+描述（文本）
- 商品图片（图像）
- 商品在图中的邻居列表

---

### 2. 文本编码器 (Text Encoder)

**设计**：采用 DistilBERT 架构，但**不使用任何预训练权重**，从头开始在 Amazon 评论文本上训练。输入为分词后的文本序列，输出为 `[CLS]` 位置的 768 维向量。

- **物品文本表示**：将商品的 `title` 和 `description` 拼接，作为该商品的文本输入。若某商品缺失描述，则仅使用标题。
  - 输入：字符串，例如 `"Girls Ballet Tutu Zebra Hot Pink"`（标题） + `"Beautiful tutu for girls, perfect for dance recitals"`（描述）
  - 输出：768 维向量 \( t_i \)

- **用户文本表示**：用户的历史评论文本（即该用户写过的所有 `reviewText`）拼接成一个长文档。考虑到长度限制，我们保留最近 5 条评论，超出则截断。
  - 输入：拼接后的字符串，例如 `"I bought this for my husband... Great purchase though!"` + `"These shoes run small..."` + ...
  - 输出：768 维向量 \( t_u \)（同样取自 `[CLS]` 位置）

---

### 3. 图像编码器 (Image Encoder)

**设计**：采用 ResNet‑50 架构，同样**从头训练**。输入为 224×224 的 RGB 图像，输出为 2048 维的全局平均池化特征。

- **物品图像表示**：下载商品主图，若无图则使用全零向量（缺失处理）。
  - 输入：图像张量（3×224×224），归一化到 [0,1]
  - 输出：2048 维向量 \( v_i \)

- **用户图像表示**：用户历史交互商品图片的集合。我们对这些图片分别通过图像编码器得到特征，然后取平均。
  - 输入：用户历史商品图片列表（数量可能不等）
  - 输出：2048 维向量 \( v_u \)（各图片特征的平均）

---

### 4. 图编码器 (Graph Encoder)

**设计**：采用 GraphSAGE（两层，聚合方式为 mean），在物品‑物品共现图上进行归纳式学习。节点初始特征为随机初始化的 128 维嵌入（可训练）。GraphSAGE 通过采样邻居并聚合信息，输出每个物品的 128 维图嵌入。

- **物品图表示**：对于任意商品 \( i \)，图编码器输出向量 \( g_i \)（128 维）。
  - 输入：整个图结构（邻接列表）及节点初始特征矩阵
  - 输出：\( g_i \in \mathbb{R}^{128} \)

- **用户图表示**：用户在图中的表示由其历史交互物品的图嵌入平均得到：
  \[
  g_u = \frac{1}{|N_u|}\sum_{i \in N_u} g_i
  \]
  其中 \( N_u \) 为用户交互过的商品集合。

---

### 5. 跨模态融合模块 (Cross‑Modal Fusion)

**设计**：采用**动态加权注意力机制**，为每个模态学习一个权重，并考虑缺失模态的情况。具体地，对于用户表示，我们有三个向量 \( t_u, v_u, g_u \)（维度可能不同，需先映射到公共空间）。我们先将它们通过全连接层映射到相同维度 \( d \)（设为 128），得到 \( \tilde{t}_u, \tilde{v}_u, \tilde{g}_u \)。然后使用注意力网络计算每个模态的权重：
\[
\alpha_m = \frac{\exp(\mathbf{w}_m^\top \tanh(\mathbf{W} \tilde{\mathbf{x}}_m + \mathbf{b}))}{\sum_{m'}\exp(\mathbf{w}_{m'}^\top \tanh(\mathbf{W} \tilde{\mathbf{x}}_{m'} + \mathbf{b}))}
\]
其中 \( \tilde{\mathbf{x}}_m \) 为某模态的映射向量，\( \mathbf{W}, \mathbf{b}, \mathbf{w}_m \) 为可训练参数。最终用户表示为加权和：
\[
\mathbf{u} = \sum_{m \in \{t,v,g\}} \alpha_m \tilde{\mathbf{x}}_m
\]
若某模态缺失（如用户历史无图片），则对应 \( \alpha_m \) 强制设为 0，其余权重重新归一化。

物品的融合方式完全相同，得到物品向量 \( \mathbf{i} \)。

- **输入**：三个模态的向量（原始维度分别为 768, 2048, 128）
- **输出**：128 维的融合向量 \( \mathbf{u} \) 和 \( \mathbf{i} \)

---

### 6. 预测层 (Prediction Layer)

**设计**：使用因子分解机（FM）计算用户 \( u \) 与物品 \( i \) 的匹配分数。FM 能够捕捉特征间的二阶交互，增强表达能力。我们将用户向量 \( \mathbf{u} \) 与物品向量 \( \mathbf{i} \) 拼接，得到 256 维特征向量 \( \mathbf{x} = [\mathbf{u}; \mathbf{i}] \)。然后：
\[
\hat{y}_{ui} = w_0 + \sum_{j=1}^{256} w_j x_j + \sum_{j=1}^{256}\sum_{k=j+1}^{256} \langle \mathbf{v}_j, \mathbf{v}_k \rangle x_j x_k
\]
其中 \( w_0, \mathbf{w}, \mathbf{V} \) 为可训练参数，\( \mathbf{v}_j \) 为因子向量（维度设为 64）。

- **输入**：\( \mathbf{u} \) (128 维) 和 \( \mathbf{i} \) (128 维)
- **输出**：标量分数 \( \hat{y}_{ui} \)

---

### 7. 训练与推理

- **训练**：使用贝叶斯个性化排序（BPR）损失，对于每个正样本（用户交互过的物品），随机采样一个负样本（未交互物品），优化目标为：
  \[
  \mathcal{L} = \sum_{(u,i,j)} -\ln \sigma(\hat{y}_{ui} - \hat{y}_{uj}) + \lambda \|\Theta\|^2
  \]
  其中 \( j \) 为负样本，\( \Theta \) 为所有参数。

- **推理**：对于给定用户 \( u \)，计算其与所有候选物品的分数，取 Top‑K 返回。

---

## 模型架构如何适应 Amazon Reviews 2023 数据集

Amazon Reviews 2023 数据集天然支持多模态学习，具体适配点如下：

| 数据集特性 | 模型适配方式 |
|-----------|-------------|
| **评论文本** (`reviewText`) | 用于构建用户文本表示（用户历史评论）以及物品的文本描述（若物品缺少描述，可用评论聚合代替）。 |
| **商品标题/描述** (`title`, `description`) | 作为物品文本表示的标准输入，比用户评论更稳定。 |
| **商品图片 URL** (`image_url`) | 通过下载图片构建图像模态，支持从头训练图像编码器。 |
| **also bought / also viewed 关系** | 构建物品‑物品图，作为图神经网络的输入，捕捉商品间的协同信号。 |
| **海量交互数据** | 模型采用归纳式图编码器（GraphSAGE），可扩展到新用户和新物品；同时使用 BPR 损失进行高效训练。 |
| **时间戳** | 用于构造时间敏感的划分，防止未来信息泄露。 |
| **多模态缺失问题** | 融合模块设计为可处理缺失模态（如图片下载失败、用户无历史图片），通过注意力机制动态调整权重。 |

---

## 具体例子

假设用户 `U001` 曾购买并评论过两个商品：
- 商品 `A`：无线耳机（评论：“音质很棒，续航长”），图片为耳机照片。
- 商品 `B`：手机壳（评论：“材质很好，但颜色偏暗”），图片为手机壳照片。

在训练完成后，我们要为 `U001` 推荐新商品 `C`：一款蓝牙音箱（标题：“便携蓝牙音箱”，描述：“防水设计，10小时续航”），图片为音箱照片，且在“also bought”图中与耳机 `A` 有边（因为买耳机的人也常买音箱）。

**步骤**：

1. **用户文本表示**：将 `U001` 的两条评论拼接成字符串，输入文本编码器 → 得到 768 维向量 \( t_u \)。
2. **用户图像表示**：将商品 `A` 和 `B` 的图片分别通过图像编码器，得到两个 2048 维向量，取平均 → \( v_u \)。
3. **用户图表示**：查询 `A` 和 `B` 的图嵌入（由 GraphSAGE 输出），取平均 → \( g_u \)。
4. **用户融合**：将 \( t_u, v_u, g_u \) 映射到公共空间，通过注意力加权得到 \( \mathbf{u} \)（假设权重为文本 0.5，图像 0.3，图 0.2）。
5. **物品文本表示**：将商品 `C` 的标题+描述输入文本编码器 → \( t_c \)。
6. **物品图像表示**：将商品 `C` 的图片输入图像编码器 → \( v_c \)。
7. **物品图表示**：查询商品 `C` 的图嵌入 → \( g_c \)。
8. **物品融合**：得到 \( \mathbf{i} \)。
9. **预测分数**：将 \( \mathbf{u} \) 和 \( \mathbf{i} \) 拼接，输入 FM → 得到分数 \( \hat{y}_{U001,C} \)。

对所有候选商品重复此过程，选择分数最高的 K 个返回给用户。

---
